STEP 1 : HOW TO CHECKOUT AND COMPILE THE CODE:
----------------------------------------------
This step applies to: BATCH, CRAB and interactive jobs.


1. Login to lxplus with SLC5 and go to your AFS working directory

ssh lxplus5
cd <your_AFS_work_area>   # NOTE: don't use /tmp for job submissions!!!


2. Compile the code:

cmsrel CMSSW_3_7_0
cd CMSSW_3_7_0/src
cmsenv
cvs co -r V00-02-01 UserCode/L1RpcTriggerAnalysis
cvs co -r V04-03-34 DQM/RPCMonitorClient
cvs co -r lumi-210710 RecoLuminosity/LumiDB
cvs co -r HEAD FWCore/PythonUtilities
cvs co PhysicsTools/PythonAnalysis  # not needed for ver >= 3_8_0 
scramv1 b clean
scramv1 b
setenv MYCODE `pwd`



STEP 2 : HOW TO CREATE .JSON FILE:
----------------------------------
This step applies to: BATCH, CRAB and interactive jobs.

The DQM Run Registry works only inside CERN network: 
http://pccmsdqm04.cern.ch/runregistry/index.iface


1. Go to RUNS sections -> Table -> Advanced query
Type-in the following criteria (**) and press APPLY:

{runNumber} in (NNNNNN,NNNNNN,...,NNNNNN) and
{groupName} = 'Collisions10' and
{datasetName} like '%Express%' and
{beamE} >= 3500 and {bfield} > 3.7 and
{cmpRpc} = 'GOOD' and
{cmpDt} = 'GOOD' and
{cmpCsc} = 'GOOD' and
{cmpMuon} = 'GOOD' and
{cmpTrack} = 'GOOD'

(**) NOTE:
For run number selection use this:
{runNumber} >= NNNNNN and {runNumber} <= NNNNNN
or:
{runNumber} = NNNNNN

For fill number selection use this:
{beamFill} in (FFFF,FFFF,...,FFFF)
or:
{beamFill} >= FFFF and {beamFill} <= FFFF
or:
{beamFill} = FFFFF


2. Then go to LUMISEC section and type-in 
the following criteria (***) in the Advanced Query:

{groupName} = 'Collisions10' and
{datasetName} like '%Express%' and
{parDcsBpix}=1 and {parDcsFpix}=1 and 
{parDcsCscPlus}=1 and {parDcsCscMinus}=1 and 
{parDcsDt0}=1 and {parDcsDtplus}=1 and {parDcsDtminus}=1 and 
{parDcsRpc}=1 and 
{parDcsTibtid}=1 and {parDcsTob}=1 and {parDcsTecP}=1 and {parDcsTecM}=1 and 
{parBeam1Stable}=1 and {parBeam2Stable}=1 and 
{parBeam1Present}=1 and {parBeam2Present}=1

(***) NOTE:
It seems that {parBeam*} criteria are correctly filled in DB 
only starting from run 139096. For previous runs simply skip 
the last 4 criteria related to the beam conditions.


3. In LUMISEC section go to Table -> Get Data -> Generate
After a while go to Table -> Get Data -> Export -> JSON 
(right-click to do Save Link As) and save the JSON file.
It's also worth to save corresponding XML file with all search criteria.



STEP 3 : HOW TO GET LIST OF FILES FROM DBS:
-------------------------------------------
This step applies to: BATCH and interactive jobs only.
Skip it for CRAB jobs.

1. From lxplus command line query DBS for .ROOT files 
corresponding to selected runs and dataset name. For instance:

dbs search --query="find file where run in ( 140160, 140159, 140158 ) and dataset.status like VALID* and dataset=/ExpressPhysics/Run2010A-Express-v4/FEVT" | grep .root > filelist_dbs


2. The list of runs in DBS query should correspond to your .JSON file.
Here is a convenient tool to display .JSON file 
(CMSSW environment from STEP1 must be set!):

printJSON.py <your_json_file> 

You can also use a run range or request a single run, eg:

dbs search --query="find file where run >= 139096 and run <= 140401 and dataset.status like VALID* and dataset = /ExpressPhysics/Run2010A-Express-v4/FEVT"
dbs search --query="find file where run = 140401 and dataset.status like VALID* and dataset = /ExpressPhysics/Run2010A-Express-v4/FEVT"

however typically one is interested only in selected runs with collisions.


3. Such 'filelist_dbs' can be used directly in 'synchroAnalysis.py'
config file for BATCH or interactive jobs, provided that .ROOT files
are stored in CASTOR (Tier-2, T2_CAF_CERN).  



STEP 4a: HOW TO RUN CODE ON CAF USING CRAB:
-------------------------------------------
This step assumes that the code is already compiled (STEP1) and
located in $MYCODE directory somewhere on AFS. 
Also input .JSON file (STEP2) must be in hand.

1. Login to lxplus with SLC5 and go to your AFS working directory.
NOTE: This working area (for job submissions) can be different 
from location of the compiled code.

ssh lxplus5
cd <your_AFS_work_area>   # NOTE: don't use /tmp for job submissions!!!


2. Initialize GRID environment/certificate:

source /afs/cern.ch/cms/LCG/LCG-2/UI/cms_ui_env.csh
cd $MYCODE ; cmsenv ; cd -
source /afs/cern.ch/cms/ccs/wm/scripts/Crab/crab.csh
voms-proxy-init -voms cms -valid 400:00


3. Create 'crab.cfg' file using template located here:
$MYCODE/UserCode/L1RpcTriggerAnalysis/test/crab.cfg_template

Replace _DATASET_NAME_ and _JSON_FILE_ placeholders into relevant
dataset name and .JSON file, respectively.
Eg. for dataset "/ExpressPhysics/Run2010A-Express-v4/FEVT"
and "GoodRuns.json" you can do it in a single step like this:

cat $MYCODE/UserCode/L1RpcTriggerAnalysis/test/crab.cfg_template | sed s%_DATASET_NAME_%"/ExpressPhysics/Run2010A-Express-v4/FEVT"% | sed s%_JSON_FILE_%GoodRuns.json% > crab.cfg
  

4. To speed-up processing long datasets you can use 'runselection' parameter
in [CMSSW] section of your 'crab.cfg'. For example:
runselection=AAAAAA-BBBBBB
or:
runselection=CCCCCC


5. Select CAF queue in [CAF] section of your 'crab.cfg' according
to the requested number of lumi sections and/or files per job.
Available queues: cmscaf1nh, cmscaf1nd, cmscaf1nw.


6.[OPTIONAL] Make sure that .ROOT files from a given dataset are available 
in CASTOR (Tier-2, T2_CAF_CERN). 
It's worth to get list of files from DBS (STEP3) and use 'nsls' command
to check if they are actually in CASTOR. Use this helper script
to convert DBS names to CASTOR names:

$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/convert_dbs_to_castor.sh `cat filelist_dbs` > filelist_castor
nsls `cat filelist_castor`


7. Copy template 'synchroAnalysis.py' configuration file:
cp $MYCODE/UserCode/L1RpcTriggerAnalysis/test/synchroAnalysis_template_crab.py synchroAnalysis.py

NOTE: Settings for 'process.source' (also .JSON) will be ignored by CRAB
and replaced by the ones from 'crab.cfg'.


8.[OPTIONAL] If needed, edit your 'synchroAnalysis.py' to:
- Change Global Tag for a given CMSSW version. More information here:
  https://twiki.cern.ch/twiki/bin/viewauth/CMS/SWGuideFrontierConditions 
- Change parameters for Filters and Analyzers used "cms.path=...",
  eg. uncomment lines for HLT selection.


9. Create and submit CRAB jobs:

crab -create
crab -validateCfg
crab -submit


10. To check status of your jobs:

crab -status
or:
bjobs


11. When CRAB jobs on CAF are completed the output will be
copied automatically to: crab_0_XXXXXXX_YYYYYY/res/out_*.tgz files. 
There is no need to do 'crab -getoutput all' as for normal GRID jobs.

NOTE: For each job, apart from compressed .TGZ files, 
also uncompressed STDOUT, STDERR, XML files will be copied.
Such files called (C*.stdout, C*.stderr, crab_*.xml) are already stored
in the compressed .TGZ files and can be safely deleted to save
disk space. In order to avoid disk overquota (eg. in your ~/scratch0)
use this workaround to periodically clean such files:

watch -n 20 `$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/clean_dir.sh crab_0*`  


STEP 4b : HOW TO RUN CODE ON CAF BATCH QUEUE:
---------------------------------------------
This step assumes that the code is already compiled (STEP1) and
located in $MYCODE directory somewhere on AFS. Also input .JSON file (STEP2) 
and input list of .ROOT files (STEP3) must be in hand.

1. Login to lxplus with SLC5 and go to your AFS working directory

ssh lxplus5
cd <your_AFS_work_area>   # NOTE: don't use /tmp for job submissions!!!

NOTE: This working area (for executing cmsRun) can be different 
from location of the compiled code.


2. Create 'synchroAnalysis.py' from the template configuration file:
cp $MYCODE/UserCode/L1RpcTriggerAnalysis/test/synchroAnalysis_template_batch.sh synchroAnalysis.py


3.[OPTIONAL] If needed, edit 'synchroAnalysis.py' template to change:
- Global Tag for a given CMSSW version. More information here:
  https://twiki.cern.ch/twiki/bin/viewauth/CMS/SWGuideFrontierConditions 
- parameters for Filters and Analyzers used "cms.path=...",
  eg. uncomment lines for HLT selection.
 

4.[OPTIONAL] If needed, edit 'submit_synchroAnalysis_batch.sh' script
located here:
$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/create_synchroAnalysis_batch.sh
to change name of the CAF queue according to the number of .ROOT per job
you want to process. Available queues: cmscaf1nh, cmscaf1nd, cmscaf1nw.


5. Create and submit BATCH jobs. 
For instance, in order to analyze 100 .ROOT files per job use:

$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/create_synchroAnalysis_batch.sh synchroAnalysis.py filelist_dbs GoodRuns.json 100
$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/submit_synchroAnalysis_batch.sh job*

6. To check status of your jobs after submission:
bjobs

To check how many jobs have already finished and have valid output:
$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/check_unfinished_batch.sh job*

NOTE: Crashed or incomplete jobs (eg. due to: CPU limit, disk quota) 
can then re-submit using:
$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/submit_synchroAnalysis_batch.sh `cat ___unfinished`



STEP 4c : HOW TO RUN CODE INTERACTIVELY (LXPLUS, CMSINTER):
-----------------------------------------------------------
This step assumes that the code is already compiled (STEP1) and
located in $MYCODE directory somewhere on AFS. Also input .JSON file (STEP2) 
and input list of .ROOT files (STEP3) must be in hand.

1. Login to lxplus with SLC5 and go to your working directory

ssh lxplus5
cd <your_work_area>   # NOTE: you can use /tmp/$USER

NOTE: This working area (for executing cmsRun) can be different 
from location of the compiled code.


2. Create 'synchroAnalysis.py' from the template configuration file
and replace _LIST_OF_FILES_ and _JSON_FILE_ placeholders with your
DBS file list and .JSON file, respectively.
For example, to in order to use "filelist_dbs" and "GoodRuns.json"
files do the following:

cat $MYCODE/UserCode/L1RpcTriggerAnalysis/test/synchroAnalysis_template_batch.sh | sed s%_LIST_OF_FILES%filelist_dbs% | sed s%_JSON_FILE_%GoodRuns.json% > synchroAnalysis.py

Alternatively, to run on single .ROOT files stored in a local disk, copy:
cp $MYCODE/UserCode/L1RpcTriggerAnalysis/test/synchroAnalysis_template_batch.sh synchroAnalysis.py
and then uncomment/comment relevant lines with 'process.source'. 


3.[OPTIONAL] Also read point 3. from STEP4b. 


4a. Short interactive jobs (~1h CPU time) can be run on lxplus.
time cmsRun synchroAnalysis.py |& tee synchroAnalysis.out


4b. Longer interactive jobs can be run on CAF 'cmsinter' queue
without time limit:
bsub -Is -q cmsinter /bin/tcsh
time cmsRun synchroAnalysis.py |& tee synchroAnalysis.out



STEP 5 : HOW TO DO FINAL PLOTS:
-------------------------------
This step assumes that STEP1-STEP4 have been done.
In particular the code is already compiled (STEP1) and
located in $MYCODE directory somewhere on AFS.


1. Go to your working directory where the plots will be created
and make sure CMSSW environment is correctly set:

cd <your_work_dir>  # you can use /tmp/$USER for that
cd $MYCODE ; cmsenv; cd -


2a. Merge outputs of CRAB jobs:

$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/merge_synchroAnalysis_crab.sh CrabDir_1 CrabDir_2 ...
$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/merge_efficiencyAnalysis_crab.sh CrabDir_1 CrabDir_2 ...
NOTE: You can use wildcards, eg. crab_0*


2b. Merge outputs of BATCH jobs:

$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/merge_synchroAnalysis_batch.sh JobDir_1 JobDir_2 ...
$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/merge_efficiencyAnalysis_batch.sh JobDir_1 JobDir_2 ...
NOTE: You can use wildcards, eg. job*


3. Create 'delays_*.txt' files for different detector parts:

$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/split_delaystxt_by_location.sh delays.txt


4. Make plots:

$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/drawSynchro.sh "CommentLine1" "CommentLine2" "PlotPrefix"
$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/drawEff.sh "CommentLine1" "CommentLine2" "PlotPrefix"

Resulting plots called 'PlotPrefix_*.gif', 'PlotPrefix_*.eps' and
'PlotPrefix*.root' will be written to subdirectories 'gif', 'eps' and 'root',
respectively.


5.[OPTIONAL] The merged 'efficiencyTree.root' file can be very long.
You can remove it after making the plots if the disk overquota is your concern.



STEP 6 : HOW TO CALCULATE INTEGRATED LUMINOSITY:
------------------------------------------------
This step assumes that STEP1-STEP4 have been done.
In particular the code is already compiled (STEP1) and
located in $MYCODE directory somewhere on AFS.

When BATCH or CRAB jobs are completed one can generate output .JSON file
with actually processed runs and lumi sections. Such .JSON files from 
several jobs can be combined together to calculate total recorded luminosity
used in the analysis. 


1a. For CRAB jobs use:

$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/generate_lumi_reports_crab.sh CrabDir_1 CrabDir_2 ...
$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/merge_lumi_reports_crab.sh CrabDir_1 CrabDir_2 ...
NOTE: You can use wildcards, eg. crab_0*


1b. For BATCH jobs use: 

$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/generate_lumi_reports_batch.sh JobDir_1 JobDir_2 ...
$MYCODE/UserCode/L1RpcTriggerAnalysis/macros/merge_lumi_reports_batch.sh JobDir_1 JobDir_2 ...
NOTE: You can use wildcards, eg. job*
NOTE: The script 'generate_lumi_reports_batch.sh' can take long time 
to execute because it has to parse STDOUT log files of all jobs. 
Therefore it's better to split this task into smaller pieces 
and run several instances of this script using interactive CAF batch queue:
bsub -Is -q cmsinter /bin/tcsh


2. At the end two files should appear in the current directory:
- lumi_report.log  <-- shows total recorded luminosity at the very end 
- lumi_report.json <-- runs and lumi sections actually analyzed by all jobs



========================
OTHER USEFUL INFORMATION
========================


DBS data discovery:
-------------------
Use webpage: https://cmsweb.cern.ch/dbs_discovery/
---> aSearch ---> .....

Or, lxplus command line:
dbs search --query="......"

Examples:

find dataset where dataset.status like VALID* and run = 138750 
     and dataset = %Commissioning/RUN2010A-MuonDPG_skim-v6%RAW-RECO%

find file,sum(file.numevents) where run = 138750 
     and dataset = %Commissioning/RUN2010A-MuonDPG_skim-v6%

find run where dataset.status like VALID* and dataset = %Muon%RECO%



CMSUI at Hoza (21 Jul 2010)
---------------------------
[na podstawie instrukcji od Tomka]

1. Po zalogowaniu na cmsui (ssh cmsui) komenda scramv1 powinna byc dostepna automatycznie. 

2. Nalezy uzywac przestrzeni dyskowej: 
/scratch/scratch0/[nazwaUzytkowanika]

3. Dostep do cernowskiego cvs na cmsui:
   Za pierwszym razem piszemy:

kinit [nazwaUzytkownika]@CERN.CH    # podajemy cernowskie haslo 

   Za kazdym nastepnym razem wystarczy samo:

kinit

4. Wykonac:
eval `scramv1 ru -[c]sh`

5. Dalej normalnie korzystamy z komendy cvs tak jak w cernie
(z mozliwoscia commitowania kodu), np:

cvs co UserCode/L1RpcTriggerAnalysis

6. Nalezy przegrac swoje certyfikaty z CERNu.
   Caly katalog ~/.globus/ do katalogu domowego na Hozej.

7. Dostep do CRABa:
source /sharesoft/Crab/CRAB_2_7_3/crab.[c]sh  # po wczesniejszym eval `scramv1 ru -[c]sh`



Scalkowana swietlnosc:
----------------------
1. Sciagnac z CVS odpowiedni pakiet:

cd CMSSW_3_7_0/src
cvs co -r lumi-210710 RecoLuminosity/LumiDB
scramv1 b
cmsenv

2. Aby zapytac o delivered/recorded lumi dla 1 runu:
lumiCalc.py -c frontier://LumiProd/CMS_LUMI_PROD -r 132440 overview

3. Aby zapytac o delivered lumi dla pliku JSON:
lumiCalc.py -c frontier://LumiProd/CMS_LUMI_PROD -i plik.json recorded

Wiecej szczegolow na:
https://twiki.cern.ch/twiki/bin/viewauth/CMS/LumiCalc



OR lub AND dwoch plikow JSON:
-----------------------------
1. Sciagnac odpowiedni pakiet:
cd CMSSW_3_7_0/src
cvs co -r HEAD FWCore/PythonUtilities
scramv1 b

2. Spod Firefox zrobic "Save as" do pliku FWCore/PythonUtilities/scripts/combine_JSON.py:
https://twiki.cern.ch/twiki/pub/CMS/Collisions2010Recipes/combine_JSON.py.txt 
a potem:
chmod +x scripts/combine_JSON.py
cd CMSSW_3_7_0/src
scramv1 b

3. Zeby zrobic OR logiczny:
./combine_JSON.py -a json_a.txt -b json_b.txt -o json_out.txt -r or

4. Zeby zrobic AND logiczny:
./combine_JSON.py -a json_a.txt -b json_b.txt -o json_out.txt -r and

Wiecej szczegolow na:
https://twiki.cern.ch/twiki/bin/viewauth/CMS/Collisions2010Recipes



Kod walidacji trygera RPC:
--------------------------

1. Zalogowac sie na lxplus z SLC5 i skompilowac kod:
ssh lxplus5
# go to your work dir
 
cmsrel CMSSW_3_7_0
cd CMSSW_3_7_0/src
cmsenv
cvs co Validation/L1Trigger
cvs co MuonAnalysis/MuonAssociators
cd Validation/L1Trigger
scramv1 b
cd test
cmsRun RPCTriggerEffTest.py

2. Wyniki w 'RPC_efficiency.root' 



Warsaw Group scratch on AFS:
----------------------------
Located at: /afs/cern.ch/cms/L1/rpc 
This directory is backed up. Group quota is 2GB.
WARNING: any member of the Warsaw Group can read/write/delete files!



Grid certificate checking:
--------------------------
In order to check if your GRID certificate is valid:
source /afs/cern.ch/cms/LCG/LCG-2/UI/cms_ui_env.csh
grid-proxy-init -debug -verify
voms-proxy-init -voms cms



To login into lxplus machine with SLC5 or SLC4:
-----------------------------------------------

For SLC5 (default):
ssh lxplus5.cern.ch 

For SLC4:
ssh lxplus4.cern.ch 



To check disk quota:
--------------------
fs listquota
